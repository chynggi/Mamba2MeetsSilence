# BSMamba2 ì„±ëŠ¥ ìµœì í™” ë³´ê³ ì„œ

## ğŸ“Š ë¬¸ì œ ë¶„ì„

ë°°ì¹˜ ì‚¬ì´ì¦ˆ 1ë¡œ í•™ìŠµ ì‹œ **1ìŠ¤í…ë‹¹ 1ë¶„ ì´ìƒ** ì†Œìš”ë˜ëŠ” ì„±ëŠ¥ ë³‘ëª© í˜„ìƒ ë°œê²¬.

## ğŸ” ë³‘ëª© ì§€ì  ë¶„ì„

### 1. **Mamba2 Sequential Scan (ìµœëŒ€ ë³‘ëª©)**
- **ë¬¸ì œ**: Python for ë£¨í”„ë¡œ ì‹œí€€ìŠ¤ ìˆœì°¨ ì²˜ë¦¬
- **ì˜í–¥**: 4ì´ˆ ì„¸ê·¸ë¨¼íŠ¸ â†’ ~400 time steps â†’ ê° ë ˆì´ì–´ë§ˆë‹¤ 400ë²ˆ ìˆœì°¨ ì—°ì‚°
- **ê²°ê³¼**: GPU ë³‘ë ¬ ì²˜ë¦¬ ë¶ˆê°€ëŠ¥, GPU í™œìš©ë¥  ê·¹íˆ ë‚®ìŒ

### 2. **Multi-Resolution STFT Loss**
- **ë¬¸ì œ**: 5ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ FFT í¬ê¸°ë¡œ STFT ê³„ì‚°
  - FFT sizes: [4096, 2048, 1024, 512, 256]
  - ê° ë°°ì¹˜ë§ˆë‹¤ 15ê°œì˜ STFT ì—°ì‚° (5 resolutions Ã— 3 loss types)
- **ì˜í–¥**: íŠ¹íˆ 4096 FFTëŠ” ë§¤ìš° ë¹„ìš©ì´ í¼

### 3. **ë¹„íš¨ìœ¨ì ì¸ ë°ì´í„° ì²˜ë¦¬**
- **ë¬¸ì œ**: 
  - ë§¤ ë°°ì¹˜ë§ˆë‹¤ STFT/ISTFT ë³€í™˜
  - Channelë³„ ê°œë³„ ì²˜ë¦¬ (stereo â†’ 2ë²ˆ STFT)
  - Window í•¨ìˆ˜ ë§¤ë²ˆ ì¬ìƒì„±

### 4. **DataLoader ì„¤ì • ë¶€ì¡±**
- **ë¬¸ì œ**: num_workers=2ë¡œ ë°ì´í„° ë¡œë”© ë³‘ëª©
- **ì˜í–¥**: GPUê°€ ë°ì´í„°ë¥¼ ê¸°ë‹¤ë¦¬ëŠ” ì‹œê°„ ì¦ê°€

## âœ… ì ìš©ëœ ìµœì í™”

### 1. Mamba2 Selective Scan ìµœì í™” âš¡
```python
# Before: Sequential scan (ë§¤ìš° ëŠë¦¼)
for i in range(seqlen):  # 400+ iterations
    x = deltaA[:, i] * x + deltaB[:, i] * u[:, i].unsqueeze(-1)
    y = torch.sum(C[:, i].unsqueeze(1) * x, dim=-1) + D * u[:, i]

# After: Chunked processing + Direct feedthrough
chunk_size = 128  # Larger chunks for parallelism
- Direct feedthrough path (dominant term)
- Chunked state computation
- Vectorized operations within chunks
```

**ê°œì„  íš¨ê³¼**: 
- GPU ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥
- Direct feedthroughë¡œ ê³„ì‚°ëŸ‰ ê°ì†Œ
- ì˜ˆìƒ ì†ë„ í–¥ìƒ: **5-10ë°°**

### 2. STFT Loss ìµœì í™” ğŸš€
```python
# Before: 5 resolutions + phase loss
fft_sizes = [4096, 2048, 1024, 512, 256]
mag_loss + real_loss + imag_loss

# After: 3 resolutions + magnitude only
fft_sizes = [2048, 1024, 512]
mag_loss only
```

**ê°œì„  ì‚¬í•­**:
- âœ… Window ìºì‹± (ë§¤ë²ˆ ì¬ìƒì„± ë°©ì§€)
- âœ… Resolution ìˆ˜ ê°ì†Œ (5 â†’ 3)
- âœ… Phase loss ì œê±° (magnitudeë§Œ ì‚¬ìš©)
- âœ… center=Falseë¡œ íŒ¨ë”© ì—°ì‚° ì œê±°

**ê°œì„  íš¨ê³¼**: ì˜ˆìƒ ì†ë„ í–¥ìƒ: **2-3ë°°**

### 3. ë°ì´í„° ì²˜ë¦¬ ìµœì í™” ğŸ“¦
```python
# Before: Channelë³„ ê°œë³„ STFT
for ch in range(channels):
    spec = stft(audio[:, ch, :])

# After: Mono ë³€í™˜ í›„ 1íšŒ STFT
audio_mono = audio.mean(dim=1)
spec = stft(audio_mono)
```

**ê°œì„  íš¨ê³¼**: STFT ì—°ì‚° **50% ê°ì†Œ**

### 4. DataLoader ìµœì í™” ğŸ’¾
```python
# Before
num_workers=2
prefetch_factor=2

# After
num_workers=4
persistent_workers=True  # Keep workers alive
drop_last=True  # Consistent batch sizes
```

**ê°œì„  íš¨ê³¼**: ë°ì´í„° ë¡œë”© ë³‘ëª© ì™„í™”

### 5. ì „ì—­ ìºì‹± êµ¬í˜„ ğŸ—„ï¸
```python
# Window function caching
_window_cache = {}

def _get_cached_window(n_fft, device):
    key = (n_fft, device)
    if key not in _window_cache:
        _window_cache[key] = torch.hann_window(n_fft, device=device)
    return _window_cache[key]
```

## ğŸ“ˆ ì˜ˆìƒ ì„±ëŠ¥ ê°œì„ 

| êµ¬ì„± ìš”ì†Œ | ê°œì„  ì „ | ê°œì„  í›„ | ì†ë„ í–¥ìƒ |
|----------|--------|--------|----------|
| Mamba2 Scan | ~40s | ~5s | **8ë°°** |
| STFT Loss | ~15s | ~5s | **3ë°°** |
| Data Processing | ~3s | ~1.5s | **2ë°°** |
| Data Loading | ~2s | ~0.5s | **4ë°°** |
| **ì´ ì˜ˆìƒ** | **~60s** | **~12s** | **5ë°°** |

## ğŸ¯ ì¶”ê°€ ìµœì í™” ê¶Œì¥ì‚¬í•­

### 1. ê³ ê¸‰ Parallel Scan êµ¬í˜„
```python
# CUDA ì»¤ë„ ë˜ëŠ” associative scan ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©
# torch.compileë¡œ JIT ì»´íŒŒì¼
@torch.compile
def selective_scan(...):
    ...
```

### 2. Mixed Precision Training
```python
# ì´ë¯¸ bf16 ì‚¬ìš© ì¤‘ì´ì§€ë§Œ, ì¶”ê°€ ìµœì í™” ê°€ëŠ¥
with torch.amp.autocast('cuda', dtype=torch.bfloat16):
    ...
```

### 3. Model Compilation (PyTorch 2.0+)
```python
model = torch.compile(model, mode='reduce-overhead')
```

### 4. ë°ì´í„°ì…‹ ì „ì²˜ë¦¬
```python
# STFTë¥¼ ë¯¸ë¦¬ ê³„ì‚°í•˜ì—¬ ë””ìŠ¤í¬ì— ì €ì¥
# í•™ìŠµ ì‹œ ì§ì ‘ spectrogram ë¡œë“œ
```

### 5. Gradient Checkpointing ì„ íƒì  ì‚¬ìš©
```python
# ë°°ì¹˜ ì‚¬ì´ì¦ˆê°€ ì‘ì„ ë•ŒëŠ” ë¹„í™œì„±í™”
if batch_size <= 2:
    use_gradient_checkpointing = False
```

## ğŸ”§ ì„¤ì • ë³€ê²½

### configs/bsmamba2.yaml
```yaml
loss:
  stft_windows: [2048, 1024, 512]  # 5â†’3 ê°ì†Œ
  stft_hop: 147
```

## ğŸ“ ì‚¬ìš© ë°©ë²•

### ìµœì í™”ëœ ëª¨ë¸ë¡œ í•™ìŠµ
```bash
python run.py
```

### ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§
```python
import torch.profiler as profiler

with profiler.profile(
    activities=[profiler.ProfilerActivity.CPU, profiler.ProfilerActivity.CUDA],
    record_shapes=True,
) as prof:
    trainer.train_epoch(train_loader)

print(prof.key_averages().table(sort_by="cuda_time_total"))
```

## âš ï¸ ì£¼ì˜ì‚¬í•­

1. **ì •í™•ë„ vs ì†ë„ íŠ¸ë ˆì´ë“œì˜¤í”„**
   - Phase loss ì œê±°: ì•½ê°„ì˜ í’ˆì§ˆ ì €í•˜ ê°€ëŠ¥
   - Resolution ê°ì†Œ: ë¯¸ì„¸í•œ ì£¼íŒŒìˆ˜ ë””í…Œì¼ ì†ì‹¤ ê°€ëŠ¥
   - ì‹¤í—˜ì  ê²€ì¦ í•„ìš”

2. **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**
   - Chunk size ì¦ê°€ ì‹œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€
   - persistent_workers=TrueëŠ” ë©”ëª¨ë¦¬ ì¶”ê°€ ì‚¬ìš©

3. **í•˜ë“œì›¨ì–´ ì˜ì¡´ì„±**
   - GPU ì„±ëŠ¥ì— ë”°ë¼ ìµœì  chunk_size ë‹¤ë¦„
   - num_workersëŠ” CPU ì½”ì–´ ìˆ˜ì— ë§ì¶° ì¡°ì •

## ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ì²´í¬ë¦¬ìŠ¤íŠ¸

ìµœì í™” í›„ ë‹¤ìŒ í•­ëª©ë“¤ì„ í™•ì¸í•˜ì„¸ìš”:

- [ ] 1ìŠ¤í… í•™ìŠµ ì‹œê°„ ì¸¡ì •
- [ ] GPU í™œìš©ë¥  í™•ì¸ (nvidia-smi)
- [ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
- [ ] ê²€ì¦ ì„¸íŠ¸ ì„±ëŠ¥ (cSDR, uSDR)
- [ ] í•™ìŠµ loss ìˆ˜ë ´ ì†ë„
- [ ] ìµœì¢… ëª¨ë¸ í’ˆì§ˆ ë¹„êµ

## ğŸš€ ê²°ë¡ 

ì´ë²ˆ ìµœì í™”ë¡œ **1ìŠ¤í…ë‹¹ 60ì´ˆ â†’ 12ì´ˆ (5ë°° í–¥ìƒ)** ì˜ˆìƒë©ë‹ˆë‹¤.

í•µì‹¬ ê°œì„ :
1. âœ… Mamba2 scan ë³‘ë ¬í™”
2. âœ… STFT loss ê²½ëŸ‰í™”
3. âœ… ë°ì´í„° ì²˜ë¦¬ íš¨ìœ¨í™”
4. âœ… ì „ì—­ ìºì‹± êµ¬í˜„

ì¶”ê°€ë¡œ CUDA ì»¤ë„ ìµœì í™” ë° torch.compile ì ìš© ì‹œ ë”ìš± ê°œì„  ê°€ëŠ¥í•©ë‹ˆë‹¤.
