# BSMamba2 Configuration File

# Model Architecture
model:
  hidden_dim: 192          # Hidden dimension for features (reduced for VRAM)
  num_layers: 4            # Number of dual-path layers (L) (reduced for VRAM)
  num_subbands: 48         # Number of frequency sub-bands (K) (reduced for VRAM)
  d_state: 64              # State dimension for Mamba2
  d_conv: 4                # Convolution size for Mamba2
  use_gradient_checkpointing: true   # Enable gradient checkpointing to save VRAM

# Audio Processing
audio:
  sample_rate: 44100       # Target sample rate
  n_fft: 2048              # FFT window size
  hop_length: 441          # Hop length for STFT (sample_rate / 100)
  segment_length: 2        # Segment length in seconds (reduced for VRAM)

# Training Configuration
training:
  batch_size: 2                      # Batch size per GPU (reduced for VRAM)
  gradient_accumulation_steps: 15    # Gradient accumulation steps (increased to maintain effective batch)
  learning_rate: 5e-4                # Initial learning rate
  num_epochs: 100                    # Total number of epochs
  precision: "bf16"                  # Training precision (bf16/fp32)
  lambda_time: 10                    # Weight for time-domain loss
  dropout: 0.0                       # Dropout rate
  output_dir: "outputs"              # Output directory for checkpoints
  resume_from: null                  # Path to checkpoint to resume from

# Loss Function
loss:
  stft_windows: [2048, 1024, 512]  # Multi-resolution STFT window sizes (reduced for speed)
  stft_hop: 147                     # Hop length for STFT loss

# Data Configuration
data:
  root: "/workspace/musdb18hq"  # Path to MUSDB18HQ dataset
